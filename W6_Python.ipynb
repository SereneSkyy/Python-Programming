{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Week 6 - Python Programming : Numpy continuation <center>\n",
    "## <center> Saurav <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification \n",
    "### Gaussian Naive Bayes Classifier\n",
    "\n",
    "### Load and prepare dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from IPython.display import display\n",
    "\n",
    "#load iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "#create new dataframe from iris data\n",
    "iris_df = pd.DataFrame(data=iris.data, columns = iris.feature_names)\n",
    "iris_df['species'] = iris.target\n",
    "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: ' virginica'})\n",
    "\n",
    "display(iris_df.head())\n",
    "x = iris_df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
    "y = iris_df['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into test and train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the GNB classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n Classification Report: \\n\",)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-means\n",
    "- MeanShift\n",
    "- DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Algorithms\n",
    "\n",
    "- Choose the number of clusters(k)\n",
    "- Initialize Cluster Centers\n",
    "- Assign Data Points to Clusters\n",
    "- Update Centroids\n",
    "- Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "home_data = pd.read_csv('data/housing.csv',usecols = ['longitude','latitude','median_house_value'])\n",
    "\n",
    "home_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Scattered plot to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.scatterplot(data = home_data, x='longitude', y='latitude', hue='median_house_value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test and Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(home_data[['latitude','longitude']], \n",
    "                                                     home_data[['median_house_value']], test_size=0.33, random_state=0)\n",
    "\n",
    "X_train_norm = preprocessing.normalize(X_train)\n",
    "X_test_norm = preprocessing.normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit the data into the K-means Model and plot the clusters into Scattered plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3, random_state =0, n_init='auto')\n",
    "kmeans.fit(X_train_norm)\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "sns.scatterplot(data = X_train, x = 'latitude', y= 'longitude', hue=kmeans.labels_)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Clustering Model using Silhoutee Score (lower score represents a better fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(X_train_norm, kmeans.labels_, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "- Precision = TP/(TP+FP)\n",
    "- Recall = TP/(TP+FN)\n",
    "- where; T is True ,F is False, P is Positive, and N is Negetive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Design simple artificial neural network\n",
    "- 1 input\n",
    "- 1 hidden layer\n",
    "- 1 output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate=0.01):\n",
    "        # initialie network parameters\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        #Random initialization of weights and biases\n",
    "        np.random.seed(42)\n",
    "        self.w1 = np.random.randn(input_neurons, hidden_neurons)\n",
    "        self.b1 = np.random.randn(hidden_neurons)\n",
    "        self.w2 = np.random.randn(hidden_neurons, output_neurons)\n",
    "        self.b2 = np.random.randn(output_neurons)\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self,x):\n",
    "        return x * ( 1 - x )\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        self.z1 = np.dot(x, self.w1) + self.b1 # w1.x +b\n",
    "        self.a1 = self.sigmoid(self.z1) # apply sigmoid activation funvtion f(z) = sigmoid(z)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2 \n",
    "        self.output = self.sigmoid(self.z2) # apply sigmoid activation function\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "    \n",
    "    def backpropagate(self, x, y_true, y_pred):\n",
    "        error_output = y_pred - y_true\n",
    "        d_output = error_output * self.sigmoid_derivative(y_pred)\n",
    "        \n",
    "        error_hidden = d_output.dot(self.w2.T)\n",
    "        d_hidden = error_hidden * self.sigmoid_derivative(self.a1)\n",
    "\n",
    "        self.w2 -= self.a1.T.dot(d_output) * self.learning_rate\n",
    "        self.b2 -= np.sum(d_output, axis=0) * self.learning_rate\n",
    "        \n",
    "        self.w1 -= x.T.dot(d_hidden) * self.learning_rate\n",
    "        self.b1 -= np.sum(d_output, axis=0) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs=5000):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward_pass(X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "\n",
    "            # Backpropagation and weights update\n",
    "            self.backpropagate(X, y, y_pred)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(self.w1, self.w2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return self.forward_pass(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some synthetic data for house sizes and corresponding prices\n",
    "np.random.seed()\n",
    "X = np.array([[500],[1000],[1500],[2000],[2500],[3000]])\n",
    "y = X * 150 + (np.random.randn(*X.shape)*10000) # Random noise\n",
    "\n",
    "nn = NeuralNetwork(input_neurons=1, hidden_neurons=1, output_neurons=1, learning_rate=0.1)\n",
    "nn.train(X,y, epochs=50000)\n",
    "\n",
    "predictions = nn.predict(X)\n",
    "\n",
    "# plotting the data\n",
    "plt.scatter(X,y, color = 'blue', label='Actual prices') #Actual prices\n",
    "plt.plot(X, predictions, color='red', label='Predicted prices') # Predicted Prices\n",
    "plt.xlabel(\"house size(sq.ft)\")\n",
    "plt.ylabel('House Price ($)')\n",
    "plt.title('House price prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # This will disable GPU usage.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize data (scale pixel values to range 0-1)\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.01 \n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer, loss ='sparse_categorical_crossentropy', metrics =['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, epochs =5, batch_size= 32)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "print(f\"Prediction for first test image: {np.argmax(predictions[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('trained-data/handwritten-digit-model.keras')\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    img =img.resize((28, 28))\n",
    "    img_array = img_array.reshape(1, 28, 28)\n",
    "    return img_array\n",
    "\n",
    "processed_image = prepare_image('data/images/6.png')\n",
    "prediction = model.predict(processed_image)\n",
    "predicted_digit = np.argmax(prediction)\n",
    "print(f'Image: 6, Predicted Digit: {predicted_digit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
